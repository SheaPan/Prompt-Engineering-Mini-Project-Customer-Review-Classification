# üìå Prompt Engineering Mini Project: Customer Review Classification

## üìù Project Overview

This project explores how different prompt designs affect the quality and consistency of Large Language Model (LLM) outputs when classifying customer reviews.

The goal is to compare a **baseline prompt (v1)** with an **improved structured prompt (v2)** for categorising customer reviews into four business-relevant categories:

- Complaint  
- Praise  
- Suggestion  
- Query  

A pre-trained open-source language model (**flan-t5-large**) was used to generate predictions.  
The project focuses on prompt design, output consistency and iterative improvement rather than model training.

###  Problem Statement

Customer reviews often contain valuable feedback, but are unstructured and time-consuming to analyse manually.

This project investigates:
- How prompt wording and structure influence model outputs
- Whether a more constrained and structured prompt can improve classification consistency
- How prompt iteration can help transform a general LLM into a practical business tool

---

## üìä Dataset and Model Used

The dataset (`sample_reviews.csv`) contains three columns:

- `id` ‚Äì review identifier  
- `review` ‚Äì customer review text  
- `sentiment` ‚Äì existing sentiment label (used for qualitative comparison)

Example:

| id | review | sentiment |
|----|--------|-----------|
| 1 | The product arrived late and was damaged. | negative |


### Model Used

- **Model:** flan-t5-large (via Hugging Face Transformers)  
- **Type:** Instruction-tuned Large Language Model  
- **Reason for selection:**  
  - Open-source  
  - No API quota required  
  - Suitable for prompt-based text classification tasks  

---

## üìÅ Folder Structure

- [PROMPT_ENGINNERING_MINI_PROJECT_CUSTOMER REVIEW CLASSIFICATION](./)
  - [code/](./code)
    - [feedback_classification.ipynb](./code/feedback_classification.ipynb)
  - [data/](./data)
    - [sample_reviews.csv](./data/sample_reviews.csv)
  - [prompts/](./prompts)
    - [prompt_v1.txt](./prompts/prompt_v1.txt)
    - [prompt_v2.txt](./prompts/prompt_v2.txt)
  - [results/](./results)
    - [output.csv](./results/output.csv)
  - [README.md](./README.md)

---

## ‚úèÔ∏è Prompt Design

### üîπ Prompt Version 1 (Baseline Prompt)

A simple instruction-based prompt with minimal structure:

```text
Classify this review into one of these categories: Complaint, Praise, Suggestion, Query.\nReview: {review}
```

**Characteristics**

- Zero-shot prompt
- Minimal constraints
- Flexible but sometimes inconsistent outputs


### üîπ Prompt Version 2 (Improved Structured Prompt)

```text
You are an assistant that classifies customer reviews.

Your task is to classify the review into exactly one of the following categories:
- Complaint
- Praise
- Suggestion
- Query

Guidelines:
- Complaint: expresses dissatisfaction or a problem
- Praise: expresses satisfaction or positive feedback
- Suggestion: gives advice or improvement ideas
- Query: asks a question or requests information

Review: "{review}"

Output only one category name from the list above. Do not add any explanation.
```

**Improvements introduced**

- Clear category definitions
- Strong output constraints
- Reduced ambiguity
- More consistent classification behaviour

---

## üõ†Ô∏è Implementation

1. Load review data using Pandas
2. Apply both prompt versions to each review
3. Store results in a comparison table
4. Export outputs to CSV for manual inspection

Output format:

| review | output_v1 | output_v2 |
|--------|-----------|-----------|

---

## üìù Evaluation Approach

Evaluation was qualitative and focused on:

- Output consistency  
- Alignment with category definitions  
- Reduction of irrelevant or unexpected responses  

Rather than formal accuracy scoring, results were reviewed manually to observe:
- Differences between prompt versions  
- Common error patterns  
- Impact of added structure in Prompt v2  

This reflects a real-world prompt engineering workflow:  

**design ‚Üí test ‚Üí observe ‚Üí refine**

---

## üîë Key Findings & Limitation

Below is an example of the comparison results generated by the two prompt versions:

| review | output_v1 | output_v2 |
|--------|-----------|-----------|
| The product arrived late and was damaged | Complaint | Complaint |
| The packaging could be better | Query | Suggestion |
| The colour is slightly different from the pictures | Query | Complaint |
| I love this item! Works perfectly | Praise | Praise |

**Findings**

- Prompt v1 sometimes produced inconsistent or vague labels  
- Prompt v2 showed improved stability due to:
  - Clear category definitions  
  - Strict output formatting rules  
- Some edge cases (mixed sentiment reviews) remained challenging, highlighting the importance of further iteration and evaluation  

**Limitations**

- No fine-tuning or model training was performed  
- Evaluation was qualitative rather than quantitative  
- Results depend on the chosen model (flan-t5-large)  

**Future improvements** could include:
- Adding few-shot examples  
- Introducing confidence scores  
- Automating evaluation metrics  
- Testing additional models  

---

## üéì What I Learned

Through this project, I learned:

- How prompt structure directly affects model behaviour  
- The importance of defining clear output constraints  
- How to iteratively refine prompts based on observed results  
- How to turn an LLM into a simple, task-oriented solution using only prompt design  

---

## üîß Technologies Used

- Python  
- Pandas  
- Hugging Face Transformers  
- flan-t5-large

---

## üíª How to Run the Script

1. Ensure Python 3 is installed

2. Install required libraries:
```python
pip install pandas transformers torch
```
3. Run the scripts
```python
python feedback_classification.ipynb
```
